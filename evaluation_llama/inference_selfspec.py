import torch
import argparse
from fastchat.utils import str_to_torch_dtype
import random
from transformers import AutoTokenizer
from evaluation_llama.eval_base import run_eval


def sample(logits, return_probs: bool = False):
    if return_probs:
        all_probs = logits.softmax(-1)
        probs, output_ids = torch.max(all_probs, dim=-1)
        return output_ids, probs

    else:
        output_ids = torch.argmax(logits, dim=-1)
        return output_ids

def selfspec_forward(input_ids, model, tokenizer, max_new_tokens, max_step_draft=12, th_stop_draft=0.8):
    step = 0
    step_verify = 0

    input_len = input_ids.shape[1]
    current_input_ids = input_ids
    generate_ids = torch.empty([input_ids.size(0), input_len + max_new_tokens + max_step_draft], dtype=torch.long,
                               device=model.device)
    generate_ids[:, :input_len] = input_ids
    draft_generate_ids = torch.empty([input_ids.size(0), max_step_draft + 1], dtype=torch.long, device=model.device)
    past_key_values = None

    n_matched = 0
    n_drafted = 0

    idx = 0
    accept_length_list = []
    with torch.no_grad():
        while True:
            idx += 1
            if step == 0:
                # first token use full model
                output = model(input_ids=current_input_ids,
                               past_key_values=past_key_values,
                               return_dict=True,
                               use_cache=True)
                logits = output['logits']
                logits = logits[:, -1:]
                output_ids = sample(logits)
                generate_ids[:, input_len + step] = output_ids
                current_input_ids = output_ids
                past_key_values = output['past_key_values']

                step += 1
                accept_length_list.append(1)

            else:
                # Draft
                draft_current_input_ids = current_input_ids
                draft_past_key_values = past_key_values
                draft_generate_ids[:, 0] = current_input_ids
                for step_draft in range(max_step_draft):
                    with model.self_draft():
                        draft_output = model(input_ids=draft_current_input_ids,
                                             past_key_values=draft_past_key_values,
                                             return_dict=True,
                                             use_cache=True)
                    draft_output_ids, draft_output_probs = sample(
                        draft_output['logits'], return_probs=True)
                    draft_generate_ids[:, step_draft + 1] = draft_output_ids
                    draft_current_input_ids = draft_output_ids
                    draft_past_key_values = draft_output['past_key_values']

                    if draft_output_probs.item() < th_stop_draft or step + step_draft + 2 >= max_new_tokens:
                        break

                drafted_n_tokens = step_draft + 1
                drafted_input_ids = draft_generate_ids[:, :drafted_n_tokens + 1]  # raft input + raft completion

                # Verify w/ the full LLM
                output = model(input_ids=drafted_input_ids,
                               past_key_values=past_key_values,
                               return_dict=True,
                               use_cache=True)
                logits = output['logits']
                output_ids = sample(logits)

                past_key_values = output['past_key_values']

                # including the one generated by the base model
                max_matched = ((output_ids[:, :-1] != drafted_input_ids[:, 1:]).cumsum(-1) == 0).sum(-1).item() + 1
                max_of_max_matched = output_ids.size(1)

                # bifurcation position
                if max_of_max_matched != max_matched:
                    output_ids = output_ids[:, :max_matched]

                    past_key_values = [
                        (k[:, :, :-(max_of_max_matched - max_matched)], v[:, :, :-(max_of_max_matched - max_matched)])
                        for k, v in past_key_values
                    ]

                generate_ids[:, input_len + step: input_len + step + output_ids.size(1)] = output_ids
                current_input_ids = output_ids[:, -1:]

                step += output_ids.size(1)
                accept_length_list.append(output_ids.size(1))

                # remove one generated by the base model
                n_matched += max_matched - 1
                n_drafted += drafted_n_tokens
                step_verify += 1

            if tokenizer.eos_token_id in output_ids[0].tolist():
                break
            if step >= max_new_tokens:
                break
    # step = min(step, max_new_tokens)
    generate_ids = generate_ids[:, :input_len + step]
    new_token_num = step
    print('token acceptance rate:', n_matched/n_drafted)
    return generate_ids, new_token_num, idx, accept_length_list, n_drafted

def generate_random_tasks(length, seed=None):
    if seed is not None:
        random.seed(seed)
    
    task_names = ['cnndm', 'gsm8k', 'tinystories', 'wmt16']
    task_flow = [random.choice(task_names) for _ in range(length)]
    
    return task_flow


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model-path",
        type=str,
        required=True,
    )
    parser.add_argument("--model-id", type=str, required=True)
    parser.add_argument("--answer-file", type=str, help="The output answer file.")
    parser.add_argument(
        "--max-new-tokens",
        type=int,
        default=50,
        help="The maximum number of new generated tokens.",
    )
    parser.add_argument(
        "--num-gpus-per-model",
        type=int,
        default=1,
        help="The number of GPUs per model.",
    )
    parser.add_argument(
        "--num-gpus-total", type=int, default=1, help="The total number of GPUs."
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="The temperature for medusa sampling.",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="float16",
        choices=["float32", "float64", "float16", "bfloat16"],
        help="Override the default dtype. If not set, it will use float16 on GPU.",
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=0.85,
        help="The top-p for sampling.",
    )
    parser.add_argument(
        "--task-name",
        type=str,
        required=True,
    )
    parser.add_argument(
        "--sub-task-name",
        type=str,
        default=None,
        help="The sub-task name for MATH task.",
    )
    parser.add_argument(
        "--mix-ratio",
        type=float,
        default=0,
    )
    parser.add_argument(
        "--data-num",
        type=int,
        default=10,
        help="The number of samples.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=2024,
        help="The sampling seed.",
    )
    parser.add_argument(
        "--max-step-draft",
        type=int,
        default=12,
        help="The maximum number of new generated tokens.",
    )

    args = parser.parse_args()

    args.model_name = ("selfspec-seed-" + str(args.seed))
    answer_file = f"results/selfspec/{args.task_name}_{args.data_num}_{args.mix_ratio}/{args.model_id}/{args.model_name}.jsonl"
    
    if args.task_name == 'mixed':
        task_names = ['cnndm', 'gsm8k', 'wmt16', 'tinystories', 'sql']
    else:
        task_names = [str(args.task_name)]
    
    if args.task_name == 'math':
        if args.sub_task_name is None:
            raise ValueError("For 'math' task, 'sub_task_name' must be specified.")
        answer_file = f"results/selfspec/{args.task_name}_{args.data_num}_{args.mix_ratio}/{args.sub_task_name}/{args.model_id}/{args.model_name}.jsonl"

    print(f"Output to {answer_file}")

    torch.nn.Linear.reset_parameters = lambda x: None

    if args.model_id.startswith("qwen"):
        from pre_inference.modeling_qwen2 import Qwen2ForCausalLM
        model = Qwen2ForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=str_to_torch_dtype(args.dtype),
            low_cpu_mem_usage=True,
            device_map="auto")
    else:
        from pre_inference.modeling_llama import LlamaForCausalLM
        model = LlamaForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=str_to_torch_dtype(args.dtype),
            low_cpu_mem_usage=True,
            device_map="auto")

    tokenizer = AutoTokenizer.from_pretrained(args.model_path)

    run_eval(
        model=model,
        tokenizer=tokenizer,
        forward_func=selfspec_forward,
        model_id=args.model_id,
        answer_file=answer_file,
        max_new_tokens=args.max_new_tokens,
        num_gpus_per_model=args.num_gpus_per_model,
        num_gpus_total=args.num_gpus_total,
        task_names=task_names,
        data_num=args.data_num,
        mix_ratio=args.mix_ratio,
        seed=args.seed,
        max_step_draft=args.max_step_draft,
        sub_task_name=args.sub_task_name,
        baseline=False
    )
